{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V9vo5R9GcBz2"
   },
   "source": [
    "## **MNIST Data Download**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JegOSoTAdWMA"
   },
   "outputs": [],
   "source": [
    "# Load and preprocess MNIST dataset using NumPy\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iSczvR8sePaY"
   },
   "source": [
    "## **Part I**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "hxCaZPdwx6aD",
    "outputId": "ce2e7cc0-984a-4c5b-c8d9-3d855d8613ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep [1/10], L: 0.0468\n",
      "Ep [2/10], L: 0.0197\n",
      "Ep [3/10], L: 0.0111\n",
      "Ep [4/10], L: 0.0091\n",
      "Ep [5/10], L: 0.0082\n",
      "Ep [6/10], L: 0.0074\n",
      "Ep [7/10], L: 0.0072\n",
      "Ep [8/10], L: 0.0066\n",
      "Ep [9/10], L: 0.0059\n",
      "Ep [10/10], L: 0.0058\n",
      "Accuracy on the test set: 94.69%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "(X_tr, y_tr), (X_t, y_t) = mnist.load_data()\n",
    "X_tr = X_tr.reshape(-1, 28*28) / 255.\n",
    "X_t = X_t.reshape(-1, 28*28) / 255.\n",
    "\n",
    "# Define neural network architecture\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, in_s, hid_s, out_s):\n",
    "        self.W1 = np.random.randn(in_s, hid_s)\n",
    "        self.b1 = np.zeros(hid_s)\n",
    "        self.W2 = np.random.randn(hid_s, out_s)\n",
    "        self.b2 = np.zeros(out_s)\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.z1 = np.dot(x, self.W1) + self.b1\n",
    "        self.a1 = self.sigmoid(self.z1)\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        return self.a2\n",
    "\n",
    "# Convert data to NumPy arrays\n",
    "in_s = X_tr.shape[1]\n",
    "hid_s = 64\n",
    "out_s = 10\n",
    "X_tr, y_tr = np.array(X_tr), np.array(y_tr)\n",
    "X_t, y_t = np.array(X_t), np.array(y_t)\n",
    "\n",
    "# Initialize the neural network\n",
    "model = NeuralNetwork(in_s, hid_s, out_s)\n",
    "\n",
    "# Define loss function\n",
    "def mse_loss(predictions, targets):\n",
    "    return np.mean((predictions - targets)**2)\n",
    "\n",
    "# Define sigmoid derivative\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Define learning rate\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "num_batches = X_tr.shape[0]\n",
    "\n",
    "# Train the model with SGD\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_l = 0.0\n",
    "    for i in range(X_tr.shape[0]):\n",
    "        # Select a random sample\n",
    "        index = np.random.randint(X_tr.shape[0])\n",
    "        input_sample = X_tr[index:index+1]\n",
    "        label_sample = y_tr[index:index+1]\n",
    "\n",
    "        # Forward pass\n",
    "        output = model.forward(input_sample)\n",
    "\n",
    "        # One-hot encode labels\n",
    "        label_onehot = np.eye(out_s)[label_sample.astype(int)]\n",
    "\n",
    "        # Compute loss\n",
    "        loss = mse_loss(output, label_onehot)\n",
    "        epoch_l += loss\n",
    "\n",
    "        # Backpropagation\n",
    "        d_loss = 2 * (output - label_onehot)\n",
    "        d_z2 = d_loss * sigmoid_derivative(model.a2)\n",
    "        d_a1 = np.dot(d_z2, model.W2.T)\n",
    "        d_z1 = d_a1 * sigmoid_derivative(model.a1)\n",
    "\n",
    "        # Update parameters\n",
    "        model.W2 -= learning_rate * np.dot(model.a1.T, d_z2)\n",
    "        model.b2 -= learning_rate * np.sum(d_z2, axis=0)\n",
    "        model.W1 -= learning_rate * np.dot(input_sample.T, d_z1)\n",
    "        model.b1 -= learning_rate * np.sum(d_z1, axis=0)\n",
    "\n",
    "    epoch_l /= X_tr.shape[0]\n",
    "    print(f\"Ep [{epoch+1}/{num_epochs}], L: {epoch_l:.4f}\")\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for i in range(0, len(X_t), batch_size):\n",
    "    inputs = X_t[i:i+batch_size]\n",
    "    labels = y_t[i:i+batch_size]\n",
    "    outputs = model.forward(inputs)\n",
    "    predicted = np.argmax(outputs, axis=1)\n",
    "    total += batch_size\n",
    "    correct += np.sum(predicted == labels)\n",
    "\n",
    "accuracy = correct / total * 100\n",
    "print(f\"Accuracy on the test set: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6bSZvMuHOCUQ"
   },
   "source": [
    "*Using the above trained model to predict this image:*\n",
    "\n",
    "![img_1.jpg](data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+vTPDHwP8TeJ9DtdXiuLCzt7kbo0uWcOU7NgKRgjkc81i+O/hvrPgW8xco1zp7ELHfIm1HYqCRjJIPUc9cHFcbSgEnABJ9BXaafH8Rrrw3NpdjBrkmjohLQLE/l7c5OOPUHgV6Fcw3um/sxXNt4hZo7qW5X7FDdLtlRfOU7QG5zgSH/dPpXhFel/Bzxj4a8H6vfzeILZy86ILe6WLzPI27i3HUZ+XkA9PQ16Pc/Hfw7pM91LaXusa20wDRxSQRQww9eAdob35DfWuNg+Ny67Dfab430SDUNLuQxjW2UK8BwcAZPPOPmyCOvPSvH6KKKK//9k=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "_EbyDAERF7a6",
    "outputId": "465d979f-011f-4b7a-e927-6ff15e555052"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [2]\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img = Image.open('img_1.jpg')\n",
    "\n",
    "# Assuming X_custom contains your own data\n",
    "X_custom = np.array(img)\n",
    "X_custom = X_custom.reshape(-1, 28*28)\n",
    "\n",
    "# Normalize your custom data (if necessary)\n",
    "X_custom_normalized = X_custom / 255.0\n",
    "\n",
    "# Create an empty array to store predictions\n",
    "predictions = []\n",
    "\n",
    "# Loop over each sample in the custom data\n",
    "for sample in X_custom_normalized:\n",
    "    # Reshape the sample to match the input size of the model\n",
    "    sample = sample.reshape(1, -1)\n",
    "\n",
    "    # Forward pass through the model\n",
    "    output = model.forward(sample)\n",
    "\n",
    "    # Get the predicted label (index of the maximum value in the output)\n",
    "    predicted_label = np.argmax(output)\n",
    "\n",
    "    # Append the predicted label to the list of predictions\n",
    "    predictions.append(predicted_label)\n",
    "\n",
    "# Convert the list of predictions to a NumPy array\n",
    "predictions = np.array(predictions)\n",
    "\n",
    "# Print the predictions\n",
    "print(\"Predictions:\", predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l_CJoHpT_Dz_"
   },
   "source": [
    "## **Pytorch Dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "AOFCJRgL-jpm"
   },
   "outputs": [],
   "source": [
    "# Setting up Pytorch imports and Data\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as opt\n",
    "import torchvision.datasets as datset\n",
    "import torchvision.transforms as trans\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Set device\n",
    "dev = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load MNIST dataset and preprocess\n",
    "trans = trans.Compose([\n",
    "    trans.ToTensor(),\n",
    "    trans.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "tr_dataset = datset.MNIST(root='./data', train=True, transform=trans, download=True)\n",
    "t_dataset = datset.MNIST(root='./data', train=False, transform=trans, download=True)\n",
    "\n",
    "tr_loader = DataLoader(tr_dataset, batch_size=batch_size, shuffle=True)\n",
    "t_loader = DataLoader(t_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aCrjkVO0lybk"
   },
   "source": [
    "## **Part II**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "LIYYDdxzU5h_",
    "outputId": "fb28b0fe-f71f-4a8e-ac68-2110266a2520"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep [1/10], L: 0.1265\n",
      "Ep [2/10], L: 0.0585\n",
      "Ep [3/10], L: 0.0505\n",
      "Ep [4/10], L: 0.0470\n",
      "Ep [5/10], L: 0.0455\n",
      "Ep [6/10], L: 0.0442\n",
      "Ep [7/10], L: 0.0433\n",
      "Ep [8/10], L: 0.0428\n",
      "Ep [9/10], L: 0.0423\n",
      "Ep [10/10], L: 0.0420\n"
     ]
    }
   ],
   "source": [
    "# Define the autoencoder architecture\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, in_s, hid_s):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(in_s, hid_s),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.decoder = nn.Linear(hid_s, in_s)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the autoencoder\n",
    "in_s = 28 * 28  # MNIST image size\n",
    "hid_s = 64\n",
    "autoenc = Autoencoder(in_s, hid_s).to(dev)\n",
    "\n",
    "# Loss and Optimizer\n",
    "c = nn.MSELoss()\n",
    "op = opt.Adam(autoenc.parameters(), lr=0.001)\n",
    "\n",
    "# Train the autoencoder\n",
    "num_epochs = 10\n",
    "for ep in range(num_epochs):\n",
    "    r_loss = 0.0\n",
    "    for img, _ in tr_loader:\n",
    "        img = img.view(img.size(0), -1).to(dev)\n",
    "\n",
    "        out = autoenc(img)\n",
    "\n",
    "        loss = c(out, img)\n",
    "\n",
    "        op.zero_grad()\n",
    "        loss.backward()\n",
    "        op.step()\n",
    "\n",
    "        r_loss += loss.item()\n",
    "\n",
    "    ep_loss = r_loss / len(tr_loader)\n",
    "    print(f\"Ep [{ep+1}/{num_epochs}], L: {ep_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-CVJ4uabpb4"
   },
   "source": [
    "## **Part III**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "AYLy1l_mbtn1",
    "outputId": "3722fa9a-0ef5-439f-e0fd-12f7b39f8b92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep [1/10], S [100/938], L: 0.0918\n",
      "Ep [1/10], S [200/938], L: 0.0869\n",
      "Ep [1/10], S [300/938], L: 0.0881\n",
      "Ep [1/10], S [400/938], L: 0.0838\n",
      "Ep [1/10], S [500/938], L: 0.0789\n",
      "Ep [1/10], S [600/938], L: 0.0718\n",
      "Ep [1/10], S [700/938], L: 0.0695\n",
      "Ep [1/10], S [800/938], L: 0.0708\n",
      "Ep [1/10], S [900/938], L: 0.0665\n",
      "Ep [2/10], S [100/938], L: 0.0626\n",
      "Ep [2/10], S [200/938], L: 0.0567\n",
      "Ep [2/10], S [300/938], L: 0.0578\n",
      "Ep [2/10], S [400/938], L: 0.0549\n",
      "Ep [2/10], S [500/938], L: 0.0550\n",
      "Ep [2/10], S [600/938], L: 0.0588\n",
      "Ep [2/10], S [700/938], L: 0.0544\n",
      "Ep [2/10], S [800/938], L: 0.0523\n",
      "Ep [2/10], S [900/938], L: 0.0510\n",
      "Ep [3/10], S [100/938], L: 0.0415\n",
      "Ep [3/10], S [200/938], L: 0.0541\n",
      "Ep [3/10], S [300/938], L: 0.0515\n",
      "Ep [3/10], S [400/938], L: 0.0463\n",
      "Ep [3/10], S [500/938], L: 0.0426\n",
      "Ep [3/10], S [600/938], L: 0.0403\n",
      "Ep [3/10], S [700/938], L: 0.0463\n",
      "Ep [3/10], S [800/938], L: 0.0475\n",
      "Ep [3/10], S [900/938], L: 0.0493\n",
      "Ep [4/10], S [100/938], L: 0.0466\n",
      "Ep [4/10], S [200/938], L: 0.0467\n",
      "Ep [4/10], S [300/938], L: 0.0554\n",
      "Ep [4/10], S [400/938], L: 0.0423\n",
      "Ep [4/10], S [500/938], L: 0.0446\n",
      "Ep [4/10], S [600/938], L: 0.0422\n",
      "Ep [4/10], S [700/938], L: 0.0452\n",
      "Ep [4/10], S [800/938], L: 0.0361\n",
      "Ep [4/10], S [900/938], L: 0.0393\n",
      "Ep [5/10], S [100/938], L: 0.0418\n",
      "Ep [5/10], S [200/938], L: 0.0411\n",
      "Ep [5/10], S [300/938], L: 0.0366\n",
      "Ep [5/10], S [400/938], L: 0.0466\n",
      "Ep [5/10], S [500/938], L: 0.0390\n",
      "Ep [5/10], S [600/938], L: 0.0396\n",
      "Ep [5/10], S [700/938], L: 0.0446\n",
      "Ep [5/10], S [800/938], L: 0.0406\n",
      "Ep [5/10], S [900/938], L: 0.0313\n",
      "Ep [6/10], S [100/938], L: 0.0471\n",
      "Ep [6/10], S [200/938], L: 0.0364\n",
      "Ep [6/10], S [300/938], L: 0.0424\n",
      "Ep [6/10], S [400/938], L: 0.0460\n",
      "Ep [6/10], S [500/938], L: 0.0351\n",
      "Ep [6/10], S [600/938], L: 0.0388\n",
      "Ep [6/10], S [700/938], L: 0.0328\n",
      "Ep [6/10], S [800/938], L: 0.0389\n",
      "Ep [7/10], S [100/938], L: 0.0388\n",
      "Ep [7/10], S [200/938], L: 0.0404\n",
      "Ep [7/10], S [300/938], L: 0.0331\n",
      "Ep [7/10], S [400/938], L: 0.0354\n",
      "Ep [7/10], S [500/938], L: 0.0389\n",
      "Ep [7/10], S [600/938], L: 0.0317\n",
      "Ep [7/10], S [700/938], L: 0.0350\n",
      "Ep [7/10], S [800/938], L: 0.0295\n",
      "Ep [7/10], S [900/938], L: 0.0309\n",
      "Ep [8/10], S [100/938], L: 0.0371\n",
      "Ep [8/10], S [200/938], L: 0.0272\n",
      "Ep [8/10], S [300/938], L: 0.0298\n",
      "Ep [8/10], S [400/938], L: 0.0288\n",
      "Ep [8/10], S [500/938], L: 0.0370\n",
      "Ep [8/10], S [600/938], L: 0.0299\n",
      "Ep [8/10], S [700/938], L: 0.0267\n",
      "Ep [8/10], S [800/938], L: 0.0335\n",
      "Ep [8/10], S [900/938], L: 0.0319\n",
      "Ep [9/10], S [100/938], L: 0.0267\n",
      "Ep [9/10], S [200/938], L: 0.0324\n",
      "Ep [9/10], S [300/938], L: 0.0333\n",
      "Ep [9/10], S [400/938], L: 0.0289\n",
      "Ep [9/10], S [500/938], L: 0.0312\n",
      "Ep [9/10], S [600/938], L: 0.0288\n",
      "Ep [9/10], S [700/938], L: 0.0332\n",
      "Ep [9/10], S [800/938], L: 0.0259\n",
      "Ep [9/10], S [900/938], L: 0.0273\n",
      "Ep [10/10], S [100/938], L: 0.0272\n",
      "Ep [10/10], S [200/938], L: 0.0320\n",
      "Ep [10/10], S [300/938], L: 0.0292\n",
      "Ep [10/10], S [400/938], L: 0.0205\n",
      "Ep [10/10], S [500/938], L: 0.0256\n",
      "Ep [10/10], S [600/938], L: 0.0247\n",
      "Ep [10/10], S [700/938], L: 0.0238\n",
      "Ep [10/10], S [800/938], L: 0.0298\n",
      "Ep [10/10], S [900/938], L: 0.0392\n",
      "Accuracy of the network on the 10000 test images: 89.18 %\n"
     ]
    }
   ],
   "source": [
    "# Define the neural network architecture\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_s, hid_s1, hid_s2, out_s):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_s, hid_s1)\n",
    "        self.fc2 = nn.Linear(hid_s1, hid_s2)\n",
    "        self.fc3 = nn.Linear(hid_s2, out_s)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sigmoid(self.fc1(x))\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Hyperparameters\n",
    "in_s = 784  # 28x28 for MNIST images\n",
    "hid_s1 = 256\n",
    "hid_s2 = 128\n",
    "out_s = 10  # 10 classes for MNIST digits\n",
    "lr = 0.2\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "model = MLP(in_s, hid_s1, hid_s2, out_s)\n",
    "\n",
    "# Training loop\n",
    "tot_stp = len(tr_loader)\n",
    "for ep in range(num_epochs):\n",
    "    for i, (img, lbl) in enumerate(tr_loader):\n",
    "        # Flatten\n",
    "        img = img.view(-1, 28*28)\n",
    "\n",
    "        # Forward pass\n",
    "        out = model(img)\n",
    "\n",
    "        # Compute MSE loss\n",
    "        c = nn.MSELoss()\n",
    "        loss = c(out, torch.nn.functional.one_hot(lbl, num_classes=out_s).float())\n",
    "\n",
    "        # Zero gradients, backward pass, and optimize\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # SGD update\n",
    "        with torch.no_grad():\n",
    "            for par in model.parameters():\n",
    "                par -= lr * par.grad\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Ep [{}/{}], S [{}/{}], L: {:.4f}'\n",
    "                   .format(ep+1, num_epochs, i+1, tot_stp, loss.item()))\n",
    "\n",
    "# Test the model\n",
    "with torch.no_grad():\n",
    "    corr = 0\n",
    "    total = 0\n",
    "    for img, lbl in t_loader:\n",
    "        img = img.view(-1, 28*28)\n",
    "        out = model(img)\n",
    "        _, predicted = torch.max(out.data, 1)\n",
    "        total += lbl.size(0)\n",
    "        corr += (predicted == lbl).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the 10000 test images: {:.2f} %'.format(100 * corr / total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VleS64sDn3cE"
   },
   "source": [
    "## **Part IV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "A1vR9Gikn7wp",
    "outputId": "3b69ee20-844a-431f-e5b4-efe37f5e0960"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder: Ep [1/10], S [100/938], L: 1.4770\n",
      "Autoencoder: Ep [1/10], S [200/938], L: 1.1810\n",
      "Autoencoder: Ep [1/10], S [300/938], L: 1.0684\n",
      "Autoencoder: Ep [1/10], S [400/938], L: 1.0222\n",
      "Autoencoder: Ep [1/10], S [500/938], L: 1.0032\n",
      "Autoencoder: Ep [1/10], S [600/938], L: 0.9807\n",
      "Autoencoder: Ep [1/10], S [700/938], L: 0.9776\n",
      "Autoencoder: Ep [1/10], S [800/938], L: 0.9693\n",
      "Autoencoder: Ep [1/10], S [900/938], L: 0.9638\n",
      "Autoencoder: Ep [2/10], S [100/938], L: 0.9585\n",
      "Autoencoder: Ep [2/10], S [200/938], L: 0.9520\n",
      "Autoencoder: Ep [2/10], S [300/938], L: 0.9516\n",
      "Autoencoder: Ep [2/10], S [400/938], L: 0.9499\n",
      "Autoencoder: Ep [2/10], S [500/938], L: 0.9467\n",
      "Autoencoder: Ep [2/10], S [600/938], L: 0.9468\n",
      "Autoencoder: Ep [2/10], S [700/938], L: 0.9458\n",
      "Autoencoder: Ep [2/10], S [800/938], L: 0.9425\n",
      "Autoencoder: Ep [2/10], S [900/938], L: 0.9428\n",
      "Autoencoder: Ep [3/10], S [100/938], L: 0.9417\n",
      "Autoencoder: Ep [3/10], S [200/938], L: 0.9372\n",
      "Autoencoder: Ep [3/10], S [300/938], L: 0.9365\n",
      "Autoencoder: Ep [3/10], S [400/938], L: 0.9417\n",
      "Autoencoder: Ep [3/10], S [500/938], L: 0.9407\n",
      "Autoencoder: Ep [3/10], S [600/938], L: 0.9411\n",
      "Autoencoder: Ep [3/10], S [700/938], L: 0.9350\n",
      "Autoencoder: Ep [3/10], S [800/938], L: 0.9370\n",
      "Autoencoder: Ep [3/10], S [900/938], L: 0.9336\n",
      "Autoencoder: Ep [4/10], S [100/938], L: 0.9369\n",
      "Autoencoder: Ep [4/10], S [200/938], L: 0.9366\n",
      "Autoencoder: Ep [4/10], S [300/938], L: 0.9398\n",
      "Autoencoder: Ep [4/10], S [400/938], L: 0.9308\n",
      "Autoencoder: Ep [4/10], S [500/938], L: 0.9385\n",
      "Autoencoder: Ep [4/10], S [600/938], L: 0.9308\n",
      "Autoencoder: Ep [4/10], S [700/938], L: 0.9340\n",
      "Autoencoder: Ep [4/10], S [800/938], L: 0.9346\n",
      "Autoencoder: Ep [4/10], S [900/938], L: 0.9329\n",
      "Autoencoder: Ep [5/10], S [100/938], L: 0.9324\n",
      "Autoencoder: Ep [5/10], S [200/938], L: 0.9353\n",
      "Autoencoder: Ep [5/10], S [300/938], L: 0.9342\n",
      "Autoencoder: Ep [5/10], S [400/938], L: 0.9311\n",
      "Autoencoder: Ep [5/10], S [500/938], L: 0.9294\n",
      "Autoencoder: Ep [5/10], S [600/938], L: 0.9298\n",
      "Autoencoder: Ep [5/10], S [700/938], L: 0.9317\n",
      "Autoencoder: Ep [5/10], S [800/938], L: 0.9290\n",
      "Autoencoder: Ep [5/10], S [900/938], L: 0.9290\n",
      "Autoencoder: Ep [6/10], S [100/938], L: 0.9319\n",
      "Autoencoder: Ep [6/10], S [200/938], L: 0.9315\n",
      "Autoencoder: Ep [6/10], S [300/938], L: 0.9310\n",
      "Autoencoder: Ep [6/10], S [400/938], L: 0.9297\n",
      "Autoencoder: Ep [6/10], S [500/938], L: 0.9310\n",
      "Autoencoder: Ep [6/10], S [600/938], L: 0.9328\n",
      "Autoencoder: Ep [6/10], S [700/938], L: 0.9308\n",
      "Autoencoder: Ep [6/10], S [800/938], L: 0.9326\n",
      "Autoencoder: Ep [6/10], S [900/938], L: 0.9309\n",
      "Autoencoder: Ep [7/10], S [100/938], L: 0.9334\n",
      "Autoencoder: Ep [7/10], S [200/938], L: 0.9297\n",
      "Autoencoder: Ep [7/10], S [300/938], L: 0.9309\n",
      "Autoencoder: Ep [7/10], S [400/938], L: 0.9309\n",
      "Autoencoder: Ep [7/10], S [500/938], L: 0.9315\n",
      "Autoencoder: Ep [7/10], S [600/938], L: 0.9298\n",
      "Autoencoder: Ep [7/10], S [700/938], L: 0.9312\n",
      "Autoencoder: Ep [7/10], S [800/938], L: 0.9284\n",
      "Autoencoder: Ep [7/10], S [900/938], L: 0.9305\n",
      "Autoencoder: Ep [8/10], S [100/938], L: 0.9277\n",
      "Autoencoder: Ep [8/10], S [200/938], L: 0.9302\n",
      "Autoencoder: Ep [8/10], S [300/938], L: 0.9297\n",
      "Autoencoder: Ep [8/10], S [400/938], L: 0.9286\n",
      "Autoencoder: Ep [8/10], S [500/938], L: 0.9317\n",
      "Autoencoder: Ep [8/10], S [600/938], L: 0.9272\n",
      "Autoencoder: Ep [8/10], S [700/938], L: 0.9315\n",
      "Autoencoder: Ep [8/10], S [800/938], L: 0.9279\n",
      "Autoencoder: Ep [8/10], S [900/938], L: 0.9291\n",
      "Autoencoder: Ep [9/10], S [100/938], L: 0.9291\n",
      "Autoencoder: Ep [9/10], S [200/938], L: 0.9296\n",
      "Autoencoder: Ep [9/10], S [300/938], L: 0.9294\n",
      "Autoencoder: Ep [9/10], S [400/938], L: 0.9290\n",
      "Autoencoder: Ep [9/10], S [500/938], L: 0.9243\n",
      "Autoencoder: Ep [9/10], S [600/938], L: 0.9298\n",
      "Autoencoder: Ep [9/10], S [700/938], L: 0.9292\n",
      "Autoencoder: Ep [9/10], S [800/938], L: 0.9306\n",
      "Autoencoder: Ep [9/10], S [900/938], L: 0.9288\n",
      "Autoencoder: Ep [10/10], S [100/938], L: 0.9299\n",
      "Autoencoder: Ep [10/10], S [200/938], L: 0.9291\n",
      "Autoencoder: Ep [10/10], S [300/938], L: 0.9302\n",
      "Autoencoder: Ep [10/10], S [400/938], L: 0.9309\n",
      "Autoencoder: Ep [10/10], S [500/938], L: 0.9292\n",
      "Autoencoder: Ep [10/10], S [600/938], L: 0.9291\n",
      "Autoencoder: Ep [10/10], S [700/938], L: 0.9292\n",
      "Autoencoder: Ep [10/10], S [800/938], L: 0.9304\n",
      "Autoencoder: Ep [10/10], S [900/938], L: 0.9285\n",
      "Fine-tuning: Ep [1/10], S [100/938], L: 0.0910\n",
      "Fine-tuning: Ep [1/10], S [200/938], L: 0.0900\n",
      "Fine-tuning: Ep [1/10], S [300/938], L: 0.0898\n",
      "Fine-tuning: Ep [1/10], S [400/938], L: 0.0899\n",
      "Fine-tuning: Ep [1/10], S [500/938], L: 0.0900\n",
      "Fine-tuning: Ep [1/10], S [600/938], L: 0.0901\n",
      "Fine-tuning: Ep [1/10], S [700/938], L: 0.0897\n",
      "Fine-tuning: Ep [1/10], S [800/938], L: 0.0901\n",
      "Fine-tuning: Ep [1/10], S [900/938], L: 0.0900\n",
      "Fine-tuning: Ep [2/10], S [100/938], L: 0.0902\n",
      "Fine-tuning: Ep [2/10], S [200/938], L: 0.0900\n",
      "Fine-tuning: Ep [2/10], S [300/938], L: 0.0896\n",
      "Fine-tuning: Ep [2/10], S [400/938], L: 0.0899\n",
      "Fine-tuning: Ep [2/10], S [500/938], L: 0.0900\n",
      "Fine-tuning: Ep [2/10], S [600/938], L: 0.0898\n",
      "Fine-tuning: Ep [2/10], S [700/938], L: 0.0900\n",
      "Fine-tuning: Ep [2/10], S [800/938], L: 0.0895\n",
      "Fine-tuning: Ep [2/10], S [900/938], L: 0.0902\n",
      "Fine-tuning: Ep [3/10], S [100/938], L: 0.0900\n",
      "Fine-tuning: Ep [3/10], S [200/938], L: 0.0899\n",
      "Fine-tuning: Ep [3/10], S [300/938], L: 0.0899\n",
      "Fine-tuning: Ep [3/10], S [400/938], L: 0.0898\n",
      "Fine-tuning: Ep [3/10], S [500/938], L: 0.0902\n",
      "Fine-tuning: Ep [3/10], S [600/938], L: 0.0900\n",
      "Fine-tuning: Ep [3/10], S [700/938], L: 0.0899\n",
      "Fine-tuning: Ep [3/10], S [800/938], L: 0.0902\n",
      "Fine-tuning: Ep [3/10], S [900/938], L: 0.0902\n",
      "Fine-tuning: Ep [4/10], S [100/938], L: 0.0899\n",
      "Fine-tuning: Ep [4/10], S [200/938], L: 0.0898\n",
      "Fine-tuning: Ep [4/10], S [300/938], L: 0.0900\n",
      "Fine-tuning: Ep [4/10], S [400/938], L: 0.0899\n",
      "Fine-tuning: Ep [4/10], S [500/938], L: 0.0900\n",
      "Fine-tuning: Ep [4/10], S [600/938], L: 0.0898\n",
      "Fine-tuning: Ep [4/10], S [700/938], L: 0.0899\n",
      "Fine-tuning: Ep [4/10], S [800/938], L: 0.0899\n",
      "Fine-tuning: Ep [4/10], S [900/938], L: 0.0899\n",
      "Fine-tuning: Ep [5/10], S [100/938], L: 0.0901\n",
      "Fine-tuning: Ep [5/10], S [200/938], L: 0.0901\n",
      "Fine-tuning: Ep [5/10], S [300/938], L: 0.0899\n",
      "Fine-tuning: Ep [5/10], S [400/938], L: 0.0902\n",
      "Fine-tuning: Ep [5/10], S [500/938], L: 0.0899\n",
      "Fine-tuning: Ep [5/10], S [600/938], L: 0.0898\n",
      "Fine-tuning: Ep [5/10], S [700/938], L: 0.0899\n",
      "Fine-tuning: Ep [5/10], S [800/938], L: 0.0900\n",
      "Fine-tuning: Ep [5/10], S [900/938], L: 0.0902\n",
      "Fine-tuning: Ep [6/10], S [100/938], L: 0.0901\n",
      "Fine-tuning: Ep [6/10], S [200/938], L: 0.0900\n",
      "Fine-tuning: Ep [6/10], S [300/938], L: 0.0904\n",
      "Fine-tuning: Ep [6/10], S [400/938], L: 0.0901\n",
      "Fine-tuning: Ep [6/10], S [500/938], L: 0.0898\n",
      "Fine-tuning: Ep [6/10], S [600/938], L: 0.0900\n",
      "Fine-tuning: Ep [6/10], S [700/938], L: 0.0901\n",
      "Fine-tuning: Ep [6/10], S [800/938], L: 0.0898\n",
      "Fine-tuning: Ep [6/10], S [900/938], L: 0.0899\n",
      "Fine-tuning: Ep [7/10], S [100/938], L: 0.0901\n",
      "Fine-tuning: Ep [7/10], S [200/938], L: 0.0901\n",
      "Fine-tuning: Ep [7/10], S [300/938], L: 0.0900\n",
      "Fine-tuning: Ep [7/10], S [400/938], L: 0.0901\n",
      "Fine-tuning: Ep [7/10], S [500/938], L: 0.0897\n",
      "Fine-tuning: Ep [7/10], S [600/938], L: 0.0900\n",
      "Fine-tuning: Ep [7/10], S [700/938], L: 0.0903\n",
      "Fine-tuning: Ep [7/10], S [800/938], L: 0.0900\n",
      "Fine-tuning: Ep [7/10], S [900/938], L: 0.0901\n",
      "Fine-tuning: Ep [8/10], S [100/938], L: 0.0900\n",
      "Fine-tuning: Ep [8/10], S [200/938], L: 0.0900\n",
      "Fine-tuning: Ep [8/10], S [300/938], L: 0.0902\n",
      "Fine-tuning: Ep [8/10], S [400/938], L: 0.0902\n",
      "Fine-tuning: Ep [8/10], S [500/938], L: 0.0898\n",
      "Fine-tuning: Ep [8/10], S [600/938], L: 0.0898\n",
      "Fine-tuning: Ep [8/10], S [700/938], L: 0.0901\n",
      "Fine-tuning: Ep [8/10], S [800/938], L: 0.0901\n",
      "Fine-tuning: Ep [8/10], S [900/938], L: 0.0902\n",
      "Fine-tuning: Ep [9/10], S [100/938], L: 0.0902\n",
      "Fine-tuning: Ep [9/10], S [200/938], L: 0.0900\n",
      "Fine-tuning: Ep [9/10], S [300/938], L: 0.0899\n",
      "Fine-tuning: Ep [9/10], S [400/938], L: 0.0899\n",
      "Fine-tuning: Ep [9/10], S [500/938], L: 0.0901\n",
      "Fine-tuning: Ep [9/10], S [600/938], L: 0.0901\n",
      "Fine-tuning: Ep [9/10], S [700/938], L: 0.0899\n",
      "Fine-tuning: Ep [9/10], S [800/938], L: 0.0898\n",
      "Fine-tuning: Ep [9/10], S [900/938], L: 0.0899\n",
      "Fine-tuning: Ep [10/10], S [100/938], L: 0.0899\n",
      "Fine-tuning: Ep [10/10], S [200/938], L: 0.0901\n",
      "Fine-tuning: Ep [10/10], S [300/938], L: 0.0900\n",
      "Fine-tuning: Ep [10/10], S [400/938], L: 0.0901\n",
      "Fine-tuning: Ep [10/10], S [500/938], L: 0.0902\n",
      "Fine-tuning: Ep [10/10], S [600/938], L: 0.0899\n",
      "Fine-tuning: Ep [10/10], S [700/938], L: 0.0901\n",
      "Fine-tuning: Ep [10/10], S [800/938], L: 0.0899\n",
      "Fine-tuning: Ep [10/10], S [900/938], L: 0.0897\n"
     ]
    }
   ],
   "source": [
    "# Define Autoencoder architecture\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, in_s, hid_s):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Linear(in_s, hid_s)\n",
    "        self.decoder = nn.Linear(hid_s, in_s)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.encoder(x))\n",
    "        x = torch.sigmoid(self.decoder(x))\n",
    "        return x\n",
    "\n",
    "# Hyperparameters\n",
    "in_s = 784  # 28x28 for MNIST images\n",
    "hid_s = 256\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "\n",
    "# Initialize the autoencoder\n",
    "autoenc = Autoencoder(in_s, hid_s)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "c = nn.MSELoss()\n",
    "op = opt.SGD(autoenc.parameters(), lr=0.1)\n",
    "\n",
    "# Training loop for the autoencoder\n",
    "tot_stp = len(tr_loader)\n",
    "for ep in range(num_epochs):\n",
    "    for i, (img, _) in enumerate(tr_loader):\n",
    "        img = img.view(-1, in_s)\n",
    "\n",
    "        # Forward pass\n",
    "        out = autoenc(img)\n",
    "        loss = c(out, img)\n",
    "\n",
    "        # Backward and optimize\n",
    "        op.zero_grad()\n",
    "        loss.backward()\n",
    "        op.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Autoencoder: Ep [{}/{}], S [{}/{}], L: {:.4f}'\n",
    "                   .format(ep+1, num_epochs, i+1, tot_stp, loss.item()))\n",
    "\n",
    "# Save the pre-trained weights of the first layer\n",
    "pretrained_weights = autoenc.encoder.weight.detach().clone()\n",
    "\n",
    "# Define the MLP architecture with pre-trained weights\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_s, hid_s1, hid_s2, out_s):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_s, hid_s1)\n",
    "        self.fc2 = nn.Linear(hid_s1, hid_s2)\n",
    "        self.fc3 = nn.Linear(hid_s2, out_s)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        # Initialize first two layers with pre-trained weights\n",
    "        with torch.no_grad():\n",
    "            self.fc1.weight = nn.Parameter(pretrained_weights)\n",
    "            self.fc1.requires_grad = False  # Freeze the parameters\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sigmoid(self.fc1(x))\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "        x = self.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "# Re-initialize the autoencoder for the fine-tuning phase\n",
    "autoenc = Autoencoder(in_s, hid_s)\n",
    "\n",
    "# Load MNIST dataset again for fine-tuning\n",
    "tr_loader = DataLoader(tr_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize the MLP model with pre-trained weights\n",
    "model = MLP(in_s, hid_s1=hid_s, hid_s2=128, out_s=10)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "c = nn.MSELoss()\n",
    "op = opt.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Training loop for fine-tuning the entire MLP\n",
    "for ep in range(num_epochs):\n",
    "    for i, (img, lbl) in enumerate(tr_loader):\n",
    "        img = img.view(-1, in_s)\n",
    "\n",
    "        # Forward pass\n",
    "        out = model(img)\n",
    "        lbl = torch.nn.functional.one_hot(lbl, num_classes=10).float()\n",
    "        loss = c(out, lbl)\n",
    "\n",
    "        # Backward and optimize\n",
    "        op.zero_grad()\n",
    "        loss.backward()\n",
    "        op.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Fine-tuning: Ep [{}/{}], S [{}/{}], L: {:.4f}'\n",
    "                   .format(ep+1, num_epochs, i+1, tot_stp, loss.item()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5YkWRF5Ivb4r"
   },
   "source": [
    "## **Part V**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sAw-RHHcvbhp",
    "outputId": "70658bca-d5e4-434a-9c1e-f1385c61a4c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  1000] loss: 1.675\n",
      "[2,  1000] loss: 0.157\n",
      "[3,  1000] loss: 0.100\n",
      "[4,  1000] loss: 0.079\n",
      "[5,  1000] loss: 0.064\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define LeNet model\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.relu(self.conv1(x))\n",
    "        x = torch.nn.functional.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        x = torch.nn.functional.relu(self.conv2(x))\n",
    "        x = torch.nn.functional.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        x = torch.nn.functional.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Initialize LeNet model\n",
    "net = LeNet()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(5):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:    # print every 1000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 1000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fT-RGKyQOJqU",
    "outputId": "0ecae64f-214a-445a-e541-49640f5a0ce6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 98.13%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Load the test dataset\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "]))\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Function to calculate accuracy\n",
    "def calculate_accuracy(net, dataloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            images, labels = data\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "# Calculate accuracy on test dataset\n",
    "accuracy = calculate_accuracy(net, testloader)\n",
    "print('Accuracy of the network on the test images: {:.2f}%'.format(accuracy * 100))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
